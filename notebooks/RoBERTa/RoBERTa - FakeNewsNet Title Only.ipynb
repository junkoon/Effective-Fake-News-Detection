{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31abbe5-1e40-402d-b5e8-c7bd4d714dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54fbfe69-7dbe-402d-8fe1-814ed70e0c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 18556\n",
      "Number of testing samples: 4640\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Directory Setup for Saving the Model and Tokenizer\n",
    "# ------------------------------------------------------------------\n",
    "SAVE_DIR = \"saved_model_fakenewsnet_roberta_title_smote\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Step 1: Load Data using Only \"title\"\n",
    "# ------------------------------------------------------------------\n",
    "# Replace these paths with your actual CSV file locations.\n",
    "fake_news = pd.read_csv('data/fake_with_metadata.csv')\n",
    "real_news = pd.read_csv('data/real_with_metadata.csv')\n",
    "\n",
    "# Work only with the title column; assign labels (1 for Fake, 0 for Real)\n",
    "fake_news = fake_news[['title']].copy()\n",
    "real_news = real_news[['title']].copy()\n",
    "fake_news['labels'] = 1\n",
    "real_news['labels'] = 0\n",
    "\n",
    "# Combine the datasets and drop any records missing title\n",
    "data = pd.concat([fake_news, real_news], ignore_index=True)\n",
    "data.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Step 2: Split Data into Training and Testing Sets\n",
    "# ------------------------------------------------------------------\n",
    "# We stratify on 'labels' to preserve class proportions.\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data['labels'])\n",
    "print(\"Number of training samples:\", len(train_df))\n",
    "print(\"Number of testing samples:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "282aef4f-15ab-412d-a3b6-4cffabc14d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Koon\\anaconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 3: Apply SMOTE Using TF-IDF Features on Training Titles\n",
    "# ------------------------------------------------------------------\n",
    "# First, vectorize the training titles using TF-IDF.\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['title']).toarray()\n",
    "y_train = train_df['labels'].values\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Define a helper function to convert numeric TF-IDF rows back into text.\n",
    "def tfidf_to_text(X):\n",
    "    texts = []\n",
    "    for row in X:\n",
    "        # inverse_transform returns a list of tokens that had nonzero TF-IDF values.\n",
    "        tokens = vectorizer.inverse_transform(row.reshape(1, -1))[0]\n",
    "        texts.append(\" \".join(tokens))\n",
    "    return texts\n",
    "\n",
    "# Convert the SMOTE-resampled training features back into text strings.\n",
    "train_texts = tfidf_to_text(X_train_resampled)\n",
    "\n",
    "# For the test set, just transform using the vectorizer (without SMOTE) and inverse transform.\n",
    "X_test_tfidf = vectorizer.transform(test_df['title']).toarray()\n",
    "test_texts = tfidf_to_text(X_test_tfidf)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(y_train_resampled, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['labels'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a996e9e8-6e29-4b2f-95ec-711d444ae610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 4: Tokenize the Titles Using RoBERTa Tokenizer\n",
    "# ------------------------------------------------------------------\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_texts(texts, max_len=128):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "train_encodings = tokenize_texts(train_texts)\n",
    "test_encodings = tokenize_texts(test_texts)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Step 5: Create a Custom Dataset Class\n",
    "# ------------------------------------------------------------------\n",
    "class TitleDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TitleDataset(train_encodings, train_labels)\n",
    "test_dataset = TitleDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d26a1707-d2cc-4928-8ac8-ac54324bfe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Koon\\anaconda3\\envs\\GPU\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 6: Load and Adapt the RoBERTa Model for Sequence Classification\n",
    "# ------------------------------------------------------------------\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 5  # Using 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f25f3b2-0090-455e-b77b-4d42cec02d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1744/1744 [01:07<00:00, 25.77it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1744/1744 [01:08<00:00, 25.63it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average Loss: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1744/1744 [01:08<00:00, 25.64it/s, loss=0.181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Average Loss: 0.2127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1744/1744 [01:08<00:00, 25.63it/s, loss=0.115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Average Loss: 0.1518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1744/1744 [01:08<00:00, 25.63it/s, loss=0.358]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Average Loss: 0.1115\n",
      "Model and tokenizer saved to saved_model_fakenewsnet_roberta_title_smote\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 7: Train the Model\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        # Move inputs to GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Model and tokenizer saved to\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07e4de5-0e19-4802-b368-661215a0730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 290/290 [00:02<00:00, 110.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8293103448275863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.87      0.91      0.89      3489\n",
      "        Fake       0.68      0.59      0.63      1151\n",
      "\n",
      "    accuracy                           0.83      4640\n",
      "   macro avg       0.77      0.75      0.76      4640\n",
      "weighted avg       0.82      0.83      0.83      4640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Step 8: Evaluate the Model\n",
    "# ------------------------------------------------------------------\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(true_labels, predictions)\n",
    "print(\"Evaluation Accuracy:\", acc)\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
