{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f66397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train texts: ['Alec Baldwin Admits He\\'s \"Bullied Women,\" Calls for a Change in Hollywood', 'Sorry Everyone, Dean Unglert Is Probably Not The Next Bachelor · Betches', \"Ashley Graham's newest swimwear line uses unedited photos\", 'Haunted Hollywood: Tinseltown Terrors, Filmdom Phantoms, and Movieland Mayhem, Second Edition: Ogden: 9781493015771: Amazon.com: Books', 'Bobbi Kristina Brown']\n",
      "Sample test texts: ['Will Bindi Irwin Get Married to Boyfriend Chandler Powell?', 'When is The Crown season 3 on Netflix? Who is in the cast, and what is going to happen?', '8 Super Relatable Products You’ll Find in Khloe Kardashian’s Purse', \"Chrissy Metz Almost Got Lube On Hugh Jackman's Suit At The MTV Movie Awards\", 'Billionaire Wissam Al Mana Sends Message About Split From Janet Jackson']\n",
      "Train encodings keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Length of input_ids: 18556\n",
      "Length of attention_mask: 18556\n",
      "Length of train_labels: 18556\n",
      "Sample labels: 9251     0\n",
      "15039    0\n",
      "15154    0\n",
      "6239     0\n",
      "11091    0\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fake News Detection Using BERT\n",
    "# Step-by-Step Implementation in a Jupyter Notebook\n",
    "\n",
    "# Step 1: Install Required Libraries\n",
    "# Uncomment the following lines if you haven't installed these libraries yet:\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install torch\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Step 3: Load Dataset (ensure the file paths are correct)\n",
    "fake_news = pd.read_csv('data/fake.csv')  # Replace with the correct path\n",
    "real_news = pd.read_csv('data/real.csv')  # Replace with the correct path\n",
    "\n",
    "# Drop the 'id' column (ensure 'id' is the column you want to remove)\n",
    "fake_news = fake_news.drop(columns=['id'], errors='ignore')  # Ignores if 'id' does not exist\n",
    "real_news = real_news.drop(columns=['id'], errors='ignore')  # Ignores if 'id' does not exist\n",
    "\n",
    "# Add labels: 1 for fake, 0 for real\n",
    "fake_news['labels'] = 1\n",
    "real_news['labels'] = 0\n",
    "\n",
    "# Combine datasets into one dataframe\n",
    "data = pd.concat([fake_news[['title', 'labels']], real_news[['title', 'labels']]], ignore_index=True)\n",
    "\n",
    "# Clean the data by removing any NaN values in the 'title' column\n",
    "data.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "# Convert the titles to strings explicitly (in case some are not strings)\n",
    "data['title'] = data['title'].astype(str)\n",
    "\n",
    "# Step 4: Split Data into Training and Testing Sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['title'], data['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure train_texts and test_texts are lists of strings\n",
    "if isinstance(train_texts, pd.Series):\n",
    "    train_texts = train_texts.tolist()\n",
    "\n",
    "if isinstance(test_texts, pd.Series):\n",
    "    test_texts = test_texts.tolist()\n",
    "\n",
    "# Print some samples to verify\n",
    "print(\"Sample train texts:\", train_texts[:5])  # Print first 5 samples\n",
    "print(\"Sample test texts:\", test_texts[:5])    # Print first 5 samples\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Debugging: Check the shape and keys of the encodings\n",
    "print(\"Train encodings keys:\", train_encodings.keys())\n",
    "print(\"Length of input_ids:\", len(train_encodings['input_ids']))\n",
    "print(\"Length of attention_mask:\", len(train_encodings['attention_mask']))\n",
    "\n",
    "# Check the structure and length of labels\n",
    "print(\"Length of train_labels:\", len(train_labels))\n",
    "print(\"Sample labels:\", train_labels[:5])  # Print the first 5 labels for inspection\n",
    "\n",
    "# Verify that the lengths match\n",
    "assert len(train_encodings['input_ids']) == len(train_labels), \\\n",
    "    f\"Length mismatch: Encodings: {len(train_encodings['input_ids'])}, Labels: {len(train_labels)}\"\n",
    "\n",
    "# Step 6: Tokenize the Data (Ensure text is in correct format)\n",
    "train_encodings = tokenize_function(train_texts)  # Now 'train_texts' is a list of strings\n",
    "test_encodings = tokenize_function(test_texts)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22850711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encodings keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Length of input_ids: 18556\n",
      "Length of attention_mask: 18556\n",
      "Length of train_labels: 18556\n",
      "Sample labels: 0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: labels, dtype: int64\n",
      "Batch input ids: torch.Size([16, 72])\n",
      "Batch labels: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3480' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3480/3480 5:54:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.356508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>0.351640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.421280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8620689655172413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.90      0.92      0.91      3481\n",
      "        Fake       0.75      0.68      0.71      1159\n",
      "\n",
      "    accuracy                           0.86      4640\n",
      "   macro avg       0.82      0.80      0.81      4640\n",
      "weighted avg       0.86      0.86      0.86      4640\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fake_news_bert_tokenizer\\\\tokenizer_config.json',\n",
       " 'fake_news_bert_tokenizer\\\\special_tokens_map.json',\n",
       " 'fake_news_bert_tokenizer\\\\vocab.txt',\n",
       " 'fake_news_bert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugging: Check the shape and keys of the encodings\n",
    "print(\"Train encodings keys:\", train_encodings.keys())\n",
    "print(\"Length of input_ids:\", len(train_encodings['input_ids']))\n",
    "print(\"Length of attention_mask:\", len(train_encodings['attention_mask']))\n",
    "\n",
    "# Check the structure and length of labels\n",
    "print(\"Length of train_labels:\", len(train_labels))\n",
    "print(\"Sample labels:\", train_labels[:5])  # Print the first 5 labels for inspection\n",
    "\n",
    "# Verify that the lengths match\n",
    "assert len(train_encodings['input_ids']) == len(train_labels), \\\n",
    "    f\"Length mismatch: Encodings: {len(train_encodings['input_ids'])}, Labels: {len(train_labels)}\"\n",
    "\n",
    "# Step 7: Create Dataset Class\n",
    "# Step 1: Ensure that the indices are properly aligned\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['title'], data['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Re-indexing the labels and texts to ensure proper alignment after splitting\n",
    "train_labels = ttrain_labels = train_labels.reset_index(drop=True)\n",
    "train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "# Step 2: Tokenize the texts and ensure proper alignment with the labels\n",
    "train_encodings = tokenize_function(train_texts.tolist())  \n",
    "test_encodings = tokenize_function(test_texts.tolist())   \n",
    "\n",
    "# Ensure proper indexing of encodings (reset the indices to align with train_labels/test_labels)\n",
    "train_encodings = {key: val[:len(train_labels)] for key, val in train_encodings.items()}\n",
    "test_encodings = {key: val[:len(test_labels)] for key, val in test_encodings.items()}\n",
    "\n",
    "# Step 3: Define the Dataset class\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "        # Ensure the lengths of encodings and labels match\n",
    "        if len(self.encodings['input_ids']) != len(self.labels):\n",
    "            raise ValueError(f\"Mismatch: encodings length ({len(self.encodings['input_ids'])}) does not match labels length ({len(self.labels)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Step 4: Create Dataset Objects for Training and Testing\n",
    "train_dataset = FakeNewsDataset(train_encodings, train_labels)\n",
    "test_dataset = FakeNewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# Step 5: Create the DataLoader with the dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Check the first batch to ensure correct data loading\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch input ids:\", batch['input_ids'].shape)\n",
    "    print(\"Batch labels:\", batch['labels'].shape)\n",
    "    break  # Check the first batch and stop\n",
    "\n",
    "\n",
    "# Step 8: Load BERT Model for Sequence Classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Step 9: Set Up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # Save the model every epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay strength\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Step 10: Set Up Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Pretrained BERT model\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset            # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Step 11: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 12: Evaluate the Model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
    "\n",
    "# Step 13: Print Evaluation Metrics\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, pred_labels))\n",
    "print(classification_report(test_labels, pred_labels, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Step 14: Save the Trained Model (Optional)\n",
    "# You can save the trained model to disk if you want to reload it later.\n",
    "model.save_pretrained('fake_news_bert_model')\n",
    "tokenizer.save_pretrained('fake_news_bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643773c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
