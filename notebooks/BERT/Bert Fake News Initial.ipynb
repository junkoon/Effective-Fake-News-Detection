{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e588fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input ids: torch.Size([16, 72])\n",
      "Batch labels: torch.Size([16])\n",
      "Batch URLs: ['www.usmagazine.com/celebrity-news/news/o-j-simpson-im-not-khloe-kardashians-dad/', 'https://people.com/tv/wells-adams-sarah-hyland-relationship-weirdly-normal/', 'www.ok.co.uk/celebrity-news/1234596/margot-robbie-pregnant-husband-tom-ackerley-expecting-baby-child-i-tonya-wolf-wall-street', 'https://www.tmz.com/2018/07/20/george-clooney-work-scooter-accident/', 'http://www.k92radio.com/news/patricia-clarkson-gets-very-candid-about-justin-timberlakes-penis-size', 'https://www.refinery29.com/en-us/2018/02/190315/julianne-moore-defends-alicia-vikander-on-set', 'hollywoodlife.com/2010/11/03/gwen-stefani-miscarriage-gavin-rossdale-national-enquirer-adoption/', 'thehill.com/policy/national-security/355749-fbi-uncovered-russian-bribery-plot-before-obama-administration', 'https://www.esquire.com/style/mens-fashion/advice/g524/kentucky-derby-attire/', 'https://medium.com/@ChristianWelch18837Uj/heres-why-paris-jackson-couldn-t-attend-the-2017-cfda-fashion-awards-after-all-4dfae4b045a2', 'https://people.com/music/joe-jackson-private-funeral/', 'http://screencrush.com/best-tv-shows-not-watching-2018/', 'celebrityinsider.org/heres-why-kendall-jenner-is-not-planning-on-becoming-a-mom-like-her-sisters-anytime-soon-98481/', 'www.eonline.com/fr/news/740766/watch-justin-bieber-awkwardly-explain-his-selena-gomez-tattoo-and-try-not-to-cringe', 'https://www.billboard.com/articles/news/bbma/7792985/ludacris-vanessa-hudgens-host-2017-billboard-music-awards', 'https://www.longroom.com/discussion/549490/new-ravens-home-trailer-is-full-of-new-catchphrases-hijinks-and-visions-of-the-future']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jun_k\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3480' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3480/3480 6:02:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.349719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360400</td>\n",
       "      <td>0.354216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.435459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.90      0.92      0.91      3481\n",
      "        Fake       0.74      0.69      0.72      1159\n",
      "\n",
      "    accuracy                           0.86      4640\n",
      "   macro avg       0.82      0.81      0.81      4640\n",
      "weighted avg       0.86      0.86      0.86      4640\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fake_news_bert_tokenizer\\\\tokenizer_config.json',\n",
       " 'fake_news_bert_tokenizer\\\\special_tokens_map.json',\n",
       " 'fake_news_bert_tokenizer\\\\vocab.txt',\n",
       " 'fake_news_bert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "fake_news = pd.read_csv('data/fake.csv')  # Replace with the correct path\n",
    "real_news = pd.read_csv('data/real.csv')  # Replace with the correct path\n",
    "\n",
    "# Drop the 'id' column (ensure 'id' is the column you want to remove)\n",
    "fake_news = fake_news.drop(columns=['id'], errors='ignore')  # Ignores if 'id' does not exist\n",
    "real_news = real_news.drop(columns=['id'], errors='ignore')  # Ignores if 'id' does not exist\n",
    "\n",
    "# Add labels: 1 for fake, 0 for real\n",
    "fake_news['labels'] = 1\n",
    "real_news['labels'] = 0\n",
    "\n",
    "# Combine datasets into one dataframe\n",
    "data = pd.concat([fake_news[['title', 'labels', 'news_url']], real_news[['title', 'labels', 'news_url']]], ignore_index=True)\n",
    "\n",
    "# Clean the data by removing any NaN values in the 'title' column\n",
    "data.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "# Convert the titles to strings explicitly (in case some are not strings)\n",
    "data['title'] = data['title'].astype(str)\n",
    "\n",
    "# Step 2: Split Data into Training and Testing Sets\n",
    "train_texts, test_texts, train_labels, test_labels, train_urls, test_urls = train_test_split(\n",
    "    data['title'], data['labels'], data['news_url'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure train_texts and test_texts are lists of strings\n",
    "if isinstance(train_texts, pd.Series):\n",
    "    train_texts = train_texts.tolist()\n",
    "\n",
    "if isinstance(test_texts, pd.Series):\n",
    "    test_texts = test_texts.tolist()\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Step 4: Create Dataset Class\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, urls):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.urls = urls\n",
    "\n",
    "        # Ensure the lengths of encodings, labels, and urls match\n",
    "        if len(self.encodings['input_ids']) != len(self.labels) or len(self.labels) != len(self.urls):\n",
    "            raise ValueError(f\"Mismatch: encodings length ({len(self.encodings['input_ids'])}) does not match labels length ({len(self.labels)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        item['news_url'] = self.urls.iloc[idx]\n",
    "        return item\n",
    "\n",
    "# Step 5: Create Dataset Objects for Training and Testing\n",
    "train_dataset = FakeNewsDataset(train_encodings, train_labels, train_urls)\n",
    "test_dataset = FakeNewsDataset(test_encodings, test_labels, test_urls)\n",
    "\n",
    "# Step 6: Create the DataLoader with the dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Check the first batch to ensure correct data loading\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch input ids:\", batch['input_ids'].shape)\n",
    "    print(\"Batch labels:\", batch['labels'].shape)\n",
    "    print(\"Batch URLs:\", batch['news_url'])  # Display URLs for the first batch\n",
    "    break  # Check the first batch and stop\n",
    "\n",
    "# Step 7: Load BERT Model for Sequence Classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Step 8: Set Up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # Save the model every epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay strength\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Step 9: Set Up Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Pretrained BERT model\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=test_dataset            # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Step 10: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 11: Evaluate the Model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
    "\n",
    "# Step 12: Print Evaluation Metrics\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, pred_labels))\n",
    "print(classification_report(test_labels, pred_labels, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Step 13: Save the Trained Model (Optional)\n",
    "# You can save the trained model to disk if you want to reload it later.\n",
    "model.save_pretrained('fake_news_bert_model')\n",
    "tokenizer.save_pretrained('fake_news_bert_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467327c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
