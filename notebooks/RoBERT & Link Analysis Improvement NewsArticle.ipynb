{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e0404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AdamW, get_scheduler\n",
    "import tldextract\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433154c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1294\n",
      "0     801\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Prepare Data\n",
    "news_data = pd.read_csv('data/news_articles.csv')  # Replace with the correct path\n",
    "\n",
    "news_data = news_data.drop(columns=['author', 'published', 'text', 'language', 'main_img_url', 'type', 'title_without_stopwords', 'text_without_stopwords', 'hasImage'], errors='ignore')\n",
    "news_data = news_data[news_data['label'].apply(lambda x: isinstance(x, str))]  # Filter out non-string labels\n",
    "news_data['label'] = news_data['label'].apply(lambda x: 1 if x.lower() == 'fake' else 0)\n",
    "\n",
    "# Check class distribution\n",
    "print(news_data['label'].value_counts())\n",
    "\n",
    "# Ensure there are both classes present\n",
    "if news_data['label'].nunique() > 1:\n",
    "    news_data.dropna(subset=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1c7b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc7aeee5c74a49ac3393f7ace91b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jun_k\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jun_k\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5903eb57538f4bb19229cc15919626a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe694107095418186e50da6c04837fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c2bd5ee81d4ec3b0b1a3fe49e5f850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bedffb8e75477485e37583b43d69e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Extract URL features (function remains the same)\n",
    "def extract_url_features(url):\n",
    "    if isinstance(url, float):\n",
    "        url = str(url)\n",
    "    ext = tldextract.extract(url)\n",
    "    return {\n",
    "        'domain': ext.domain,\n",
    "        'subdomain': ext.subdomain,\n",
    "        'url_length': len(url),\n",
    "        'has_query': 1 if '?' in url else 0,\n",
    "        'has_hyphens': 1 if '-' in url else 0,\n",
    "        'has_numbers': 1 if any(c.isdigit() for c in url) else 0,\n",
    "    }\n",
    "\n",
    "# Assuming news_data is already prepared (filtered and augmented as before)\n",
    "\n",
    "url_features = news_data['site_url'].apply(extract_url_features)\n",
    "url_features_df = pd.DataFrame(url_features.tolist())\n",
    "\n",
    "# Encode and normalize URL features\n",
    "url_features_encoded = pd.get_dummies(url_features_df, columns=['domain', 'subdomain'])\n",
    "scaler = StandardScaler()\n",
    "url_features_encoded = pd.DataFrame(scaler.fit_transform(url_features_encoded), columns=url_features_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fa9f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jun_k\\anaconda3\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jun_k\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorize Titles\n",
    "vectorizer = CountVectorizer(max_features=5000)  # Limit features to avoid memory issues\n",
    "title_features_vectorized = vectorizer.fit_transform(news_data['title']).toarray()\n",
    "\n",
    "# Align Data\n",
    "min_length = min(len(title_features_vectorized), len(url_features_encoded))\n",
    "title_features_vectorized = title_features_vectorized[:min_length]\n",
    "url_features_encoded = url_features_encoded.iloc[:min_length]\n",
    "news_data = news_data.iloc[:min_length]\n",
    "\n",
    "# Combine Features for SMOTE\n",
    "combined_features = np.hstack([title_features_vectorized, url_features_encoded])\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "combined_features_resampled, labels_resampled = smote.fit_resample(combined_features, news_data['label'])\n",
    "\n",
    "# Split Combined Features Back\n",
    "title_features_resampled = combined_features_resampled[:, :title_features_vectorized.shape[1]]\n",
    "url_features_resampled = combined_features_resampled[:, title_features_vectorized.shape[1]:]\n",
    "\n",
    "# Convert resampled title features back to text\n",
    "title_features_resampled = [\n",
    "    \" \".join(vectorizer.inverse_transform(title_row.reshape(1, -1))[0]) for title_row in title_features_resampled\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d505bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train/Test Sets\n",
    "train_texts, test_texts, train_labels, test_labels, train_urls, test_urls = train_test_split(\n",
    "    title_features_resampled, labels_resampled, url_features_resampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the train and test texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_encodings = {key: torch.tensor(val) for key, val in train_encodings.items()}\n",
    "test_encodings = {key: torch.tensor(val) for key, val in test_encodings.items()}\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, url_features):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.url_features = torch.tensor(url_features, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        item['url_features'] = self.url_features[idx]\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CombinedDataset(train_encodings, train_labels, train_urls)\n",
    "test_dataset = CombinedDataset(test_encodings, test_labels, test_urls)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236e1bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d968c296c64a5092a6276bc509010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model with enhanced URL features\n",
    "class RobertaWithEnhancedURLClassifier(nn.Module):\n",
    "    def __init__(self, roberta_model_name, url_feature_dim):\n",
    "        super(RobertaWithEnhancedURLClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.url_transform = nn.Sequential(\n",
    "            nn.Linear(url_feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 64, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, url_features):\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        url_transformed = self.url_transform(url_features)\n",
    "        combined_input = torch.cat((roberta_output, url_transformed), dim=1)\n",
    "        logits = self.classifier(combined_input)\n",
    "        return logits\n",
    "\n",
    "url_feature_dim = train_urls.shape[1]\n",
    "model = RobertaWithEnhancedURLClassifier('roberta-base', url_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e10ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jun_k\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf68abad0a14463873579ef915b2098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Evaluation Loss: 0.6266617802056399\n",
      "Epoch 2/5, Evaluation Loss: 0.5564503136909369\n",
      "Epoch 3/5, Evaluation Loss: 0.5080450050758593\n",
      "Epoch 4/5, Evaluation Loss: 0.529389812187715\n",
      "Epoch 5/5, Evaluation Loss: 0.5332108853441296\n",
      "Model saved as fake_news_model_newsarticle.pth\n"
     ]
    }
   ],
   "source": [
    "# Model training function remains the same\n",
    "def train_model(model, train_dataloader, eval_dataloader, num_epochs, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    model.to(device)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                url_features=batch[\"url_features\"]\n",
    "            )\n",
    "            logits = outputs\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5], device=device))\n",
    "            loss = loss_fct(logits, batch[\"labels\"])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    url_features=batch[\"url_features\"]\n",
    "                )\n",
    "                logits = outputs\n",
    "                loss = loss_fct(logits, batch[\"labels\"])\n",
    "                eval_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Evaluation Loss: {eval_loss / len(eval_dataloader)}\")\n",
    "    return model\n",
    "\n",
    "# Train Model\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 5\n",
    "model = train_model(model, train_dataloader, eval_dataloader, num_epochs, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, \"fake_news_model_newsarticle.pth\")\n",
    "print(\"Model saved as fake_news_model_newsarticle.pth\")\n",
    "\n",
    "# Evaluation remains the same\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            url_features=batch[\"url_features\"]\n",
    "        )\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Save predictions and true labels (optional)\n",
    "np.save(\"predictions_newsarticle.npy\", predictions)\n",
    "np.save(\"true_labels_newsarticle.npy\", true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1e39ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEGCAYAAAAHRgwvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeeElEQVR4nO3de7xc0/3/8df7JEEIOYlcRIK4hDZ8iQiC75eQflv0EpSWUpdSdW+1qnzbovy01Sqta8U9rSYSlFD3W4M2iEsiEpq0LgkhIsQlEZJ8fn/sfWKczMyZOZnZc+bk/cxjHmfvtfesWScjH2vttddnKyIwM7MVNdS6AWZmbZUDpJlZAQ6QZmYFOECamRXgAGlmVkDHWjegkrR6l1DndWvdDCvD1pv0qHUTrAyzXnmFt9+ep5Wpo8M6G0UsWVTSubHorXsiYs+V+byV0b4CZOd1WX3YT2vdDCvDvX86otZNsDJ8cbehK11HLPmI1T93YEnnfvTMxTX9P2i7CpBmVgcEaKU6oZlxgDSz7Kk+pj8cIM0se+5BmpnlI2joUOtGlMQB0syyJTzENjPLTx5im5kV5B6kmVkB7kGameWjuulB1kcrzaz9EMksdimvlqqSNpD0kKRpkp6X9P20vLuk+yTNSH92S8sl6SJJMyVNkTS4WP0OkGaWsbQHWcqrZUuAH0XEQGAocLykgcBpwAMRMQB4IN0H2AsYkL6OBi4vVrkDpJllr0GlvVoQEXMi4ul0+31gOtAXGAFcn552PbBPuj0CGBWJiUCjpD6F6vc1SDPLVnn3QfaQNClnf2REjMxbrdQf2BZ4HOgdEXPSQ28AvdPtvsCsnLfNTsvmkIcDpJllr/RZ7HkRMaTl6tQFuBn4QUS8p5z6IyIkterphA6QZpaxyi41lNSJJDjeEBG3pMVvSuoTEXPSIfTctPw1YIOct/dLy/LyNUgzy16FJmmUdBWvBqZHxAU5h8YDh6XbhwG35ZQfms5mDwUW5AzFV+AepJllSxVdargL8G3gOUnPpmX/B/waGCvpSOAV4BvpsTuBvYGZwEKgaMZmB0gzy16FbhSPiEdJpn3yGZ7n/ACOL7V+B0gzy56XGpqZ5VM/Sw0dIM0sW01LDeuAA6SZZcw9SDOzwnwN0sysAPcgzcwKcA/SzCwP+RqkmVlBanCANDNbgQB5iG1mlocovDiwjXGANLOMyT1IM7NCHCDNzApo8CSNmVkevgZpZpaffA3SzKwwB0gzswIcIM3MCqiXAFkfU0lm1n4I1KCSXi1WJV0jaa6kqTllgyRNlPSspEmSdkjLJekiSTMlTZE0uKX6HSDNLFNNkzSlvEpwHbBns7LfAL+IiEHAGek+wF7AgPR1NHB5S5U7QJpZ5ioVICNiAjC/eTGwTrrdFXg93R4BjIrERKBRUp9i9fsapJllr/RLkD0kTcrZHxkRI1t4zw+AeySdT9IJ3Dkt7wvMyjlvdlo2p1BFDpBmli2VNUkzLyKGlPkJxwInR8TNkr4BXA18ocw6AA+xzawGKngNMp/DgFvS7XHADun2a8AGOef1S8sKcoA0s0wJ0dDQUNKrlV4Hdku39wBmpNvjgUPT2eyhwIKIKDi8Bg+xzawWKnQbpKTRwDCSa5WzgTOB7wJ/kNQR+IhkxhrgTmBvYCawEDiipfodIM0sW+VdgywqIg4qcGi7POcGcHw59TtAmlnm6mUljQOkmWXOAdLMrIBSlhG2BZ7FbgMuPm43/nX1t/nHBfsvL9tqo+7cc+4IHvvd/ow+7Uus3bkTAN26rM74s77CrD8dwW+O3KVWTbYcC95fxDE/v5Y9DvkVexzyK56a+vLyYyPHPMRGu57M/Hc/qF0D25hSb/FpC73MqgZISUvTBeNTJd0uqbGV9Rwu6ZIKN6/NGP3Qi+z//+78TNkfjt2VX9zwBLv86CbueOJlThyxDQCLP1nKL8c8yRl/mliLploev7joFnbb8fM8+OfTufvaH7PZRr0BeP3Nd3jkyRfp27tbjVvY9jhAJhZFxKCI2IpkvWRZM0irin9Mf4N3Plj8mbLN+jTyj2nJLVoPT57NV3fcGICFi5cw8YU3+ejjpZm301b03geLeHzyfzjwyzsCsFqnjnRduzMAZ19yK6cf+1XawL/zNscBckX/JFn3iKRNJd0t6SlJj0j6XFr+VUmPS3pG0v2SemfYvjblhdnz2Xv7jQAYsdMm9O2xVo1bZPnMmjOfdRu7cMqvRrPXkedz6nljWLhoMfc+8hzr9ejKwM361rqJbZNKfNVYJgFSUgdgOMmd7AAjgRMjYjvgFOCytPxRYGhEbAuMAU4toe6j05xvk+Lj9nOd54RL/86Re27JQ+ftS5fOnfhkybJaN8nyWLp0KVNnzOaQfXbhrqtPYc01VuPCa+/h0j/fzw+P3KvWzWuz6qUHWe1Z7M6SniXpOU4H7pPUhSS7xricv4DV05/9gBvTFESrAS+19AFpZo+RAA2NG0VFW19DM15fwNfPSa5LbtqnK18cvGGNW2T5rNezkT49u7LtwKS3v/ewbbjw2nuYNWc+e33ntwDMeWsBXz7qd9x2xcn0WnedYtWtEiRoqJNZ7GoHyEURMUjSmsA9JNcgrwPeTZNZNncxcEFEjJc0DDiryu1rs3qsswbz3vsICU7Zf1uuvW96rZtkefRadx369Grk36/OZdMNe/HYUzPYavN+jP79ccvP2eUbZ3P7yB/SvbFLDVvalrSN3mEpMrkPMiIWSjoJuJVkOP2SpAMiYpySv6mtI2IySXLLpuwah2XRtrbgqh/swS5brs+6a6/B1Cu+xa9vfIq11ujEUXsOBOCOx1/mhgdfXH7+5MsOYu3OnejUsQN777ARXz/nTl6c/W6NWm+/+P7X+f45f+KTT5ay4frrcv7phVa/WZM6iY/Z3SgeEc9ImgIcBBwMXC7pZ0AnkuuNk0l6jOMkvQM8CGycVftq6ajfP5i3/Io7p+Yt3+a40dVsjpVpywF9uePKHxU8/tjYMzJsTX1wDxKIiC7N9r+as9v8ORJExG3AbXnKryMZmptZvZN7kGZmeQlP0piZFeQAaWaWj4fYZmb5CU/SmJkVUD/3QTrdmZllTirt1XI9ukbSXElTm5WfKOkFSc9L+k1O+emSZkp6UdKXWqrfPUgzy1ZllxpeB1wCjFpevbQ7MALYJiIWS+qVlg8EDgS2BNYH7pe0eUQUTI3lHqSZZarpGmQlklVExASSVIq5jgV+HRGL03PmpuUjgDERsTgiXiJ5uuEOFOEAaWaZq9QQu4DNgf9JUyf+XdL2aXlfYFbOebPTsoI8xDazzJUxSdND0qSc/ZFpBq9iOgLdgaHA9sBYSZuU30oHSDOrgTJ6h/MiYkiZ1c8Gbkmfg/2EpGVAD5JEOBvknNePT5Pj5OUhtpllS1VPmHsrsDuApM1JcsvOI0nYfaCk1SVtDAwAnihWkXuQZpYpoYrNYksaDQwjGYrPBs4ErgGuSW/9+Rg4LO1NPi9pLDANWAIcX2wGGxwgzawGKnWfeEQUSr55SIHzzwXOLbV+B0gzy1y9rKRxgDSzbDlZhZlZfk5WYWZWhAOkmVkBTphrZpaPr0GameWnOsoH6QBpZpmrk/joAGlm2WuokwjpAGlmmVJlE+ZWlQOkmWWuTuKjA6SZZa/uJ2kkXQxEoeMRcVJVWmRm7V6dxMeiPchJRY6ZmbWKSG71qQcFA2REXJ+7L2nNiFhY/SaZWXtXL9cgW8woLmknSdOAF9L9bSRdVvWWmVn7pCRhbimvWivlkQu/B74EvA0QEZOBXavYJjNrx0RyH2Qpr1oraRY7ImY1m3UqmqbczKyYNhD7SlJKgJwlaWcgJHUCvg9Mr26zzKw9q5fbfEoZYh8DHE/ygO3XgUHpvplZ2aTSXy3XpWskzU0f0NX82I8khaQe6b4kXSRppqQpkga3VH+LPciImAcc3HJTzcxK06FyPcjrgEuAUbmFkjYAvgi8mlO8F8mjXgcAOwKXpz8LKmUWexNJt0t6K43Ut0napKxfwcwsR6Weix0RE4D5eQ5dCJzKZxe7jABGRWIi0CipT7H6Sxli/wUYC/QB1gfGAaNLeJ+Z2QqSWezSXiTPu56U8zq6xfqlEcBr6R03ufoCs3L2Z6dlBZUySbNmRPwpZ//Pkn5cwvvMzFZUYu8wNS8ihpRetdYE/o9keL3Siq3F7p5u3iXpNGAMSXf1m8CdlfhwM1s1VXESe1NgY2ByGoT7AU9L2gF4Ddgg59x+aVlBxXqQT5EExKZf5Xs5xwI4vaxmm5mlqnWbT0Q8B/TK+ZyXgSERMU/SeOAESWNIJmcWRMScYvUVW4u9cWWabGb2KQEdKrSMUNJoYBjJtcrZwJkRcXWB0+8E9gZmAguBI1qqv6SVNJK2AgYCazSVRcSowu8wMyusUv3HiDioheP9c7aDMu/hbjFASjqTJEIPJInAewGP0uy+IzOzUkj180yaUm7z2R8YDrwREUcA2wBdq9oqM2vXKrWSptpKGWIviohlkpZIWgeYy2dngszMylIva7FLCZCTJDUCV5LMbH8A/LOajTKz9q1O4mNJa7GPSzf/KOluYJ2ImFLdZplZeyWpYrPY1VbsRvGCmS4kDY6Ip6vTJDNr79rDEPt3RY4FsEeF27LStt20J4/d1OJSTWtDum1/Qq2bYGVY/OKslk8qQSmzw21BsRvFd8+yIWa2ahDtowdpZlYVdXIJ0gHSzLIlVW6pYbU5QJpZ5uokPpaUUVySDpF0Rrq/YZo6yMysVeplJU0pk0mXATsBTYvC3wcurVqLzKxda2/Pxd4xIgZLegYgIt6RtFqV22Vm7Vjd3+aT4xNJHUgffiOpJ7Csqq0ys3atDXQOS1JKgLwI+CvQS9K5JNl9flbVVplZu9Uulho2iYgbJD1FkvJMwD4RMb3qLTOzdqtO4mNJCXM3JElPfntuWUS8WvhdZmb5NU3S1INShth/49OHd61B8sSwF4Etq9guM2vH6iQ+tjyZFBH/FRFbpz8HADvgfJBm1lpKhtilvFqsSrpG0lxJU3PKfivpBUlTJP01zWfbdOx0STMlvSjpSy3VX/Zse5rmbMdy32dm1kQl/inBdcCezcruA7aKiK2Bf5E+olrSQOBAktHvnsBl6R06BZVyDfKHObsNwGDg9VJabmbWnICOFboRMiImSOrfrOzenN2JJHfeAIwAxkTEYuAlSTNpYURcyjXItXO2l5Bck7y5hPeZmeVVRrqzHpIm5eyPjIiRZXzUd4Ab0+2+JAGzyey0rKCiATLtfq4dEaeU0SAzs4KSWeyST58XEUNa9TnST0k6dTe05v1Q/JELHSNiiaRdWlu5mdkKMkhEIelw4CvA8IiItPg1PvtE1n5pWUHFepBPkFxvfFbSeGAc8GHTwYi4pfxmm5lV9z5ISXsCpwK7RcTCnEPjgb9IugBYHxhAEucKKuUa5BrA2yTPoGm6HzIAB0gzK5uADhWapJE0GhhGcq1yNnAmyaz16sB96bXOiRFxTEQ8L2ksMI1k6H18RCwtVn+xANkrncGeyqeBsUnkf4uZWUtEQ2m38LQoIg7KU3x1kfPPBc4ttf5iAbID0AXy/iYOkGbWKslDu2rditIUC5BzIuLszFpiZquGElfJtAXFAmSd/ApmVm/aQ7KK4Zm1wsxWGe1iiB0R87NsiJmtOtpNwlwzs0oS7euZNGZmlaOy1mLXlAOkmWWuPsKjA6SZZay9PXLBzKyi6iM8OkCaWeZEg2exzcxW5FlsM7MiPIttZlZAfYRHB0gzy5rvgzQzy09ABwdIM7P86iM8OkCaWQ3USQfSAdLMspXc5lMfEbJebkcys3ZEKu3Vcj26RtJcSVNzyrpLuk/SjPRnt7Rcki6SNFPSFEmDW6rfAdLMMqaS/5TgOmDPZmWnAQ9ExADggXQfYC+SR70OAI4GLm+pcgdIM8tU0yx2Ka+WRMQEoHly7xHA9en29cA+OeWjIjERaJTUp1j9vgZpZtkqcfic6iFpUs7+yIgY2cJ7ekfEnHT7DaB3ut0XmJVz3uy0bA4FOECaWebKCJDzImJIaz8nIkJSqx9T7SG2mWWugtcg83mzaeic/pyblr8GbJBzXr+0rCAHSDPLVJIwt7RXK40HDku3DwNuyyk/NJ3NHgosyBmK5+UhtpllrlIZxSWNBoaRXKucDZwJ/BoYK+lI4BXgG+npdwJ7AzOBhcARLdXvAGlmmVuJ4fNnRMRBBQ4Nz3NuAMeXU78DZBu09dfOoMuaq9OhoYGOHRt4aNRPOPfyO7hzwhQaJHp2X5tLzzyEPj0ba93UVVLf3o1cftah9Oy+NgFc/9fHuGLMw4wYvi0/OXpvtujfm+GHn8+z019d/p6TD/8ih3xtJ5YuW8Zp59/EgxOn1+4XqLGmIXY9qFqAlLQUeC6naJ+IeDnPef2BOyJiq2q1pR7d/sfvs25jl+X7J357OD899isAXDHmYX5z1V1ceHqh/3laNS1Zsoyf/f4Wprw4my5rrs5Do37Cw4+/wPR/v86hp165wveyxcbrsd//Dmanb57Lej27cuulJzDk62ezbFmrJ1fr3EpNwGSqmj3IRRExqIr1r1LW6dJ5+faHixbXTT699ujNt9/jzbffA+CDhYv518tv0KdnIw8/8ULe8/febWtuue9pPv5kCa++/jb/mTWP7bbsz5PPvZRls9uO8u6DrKnMhtiSupDMJnUDOgE/i4jbmp2zCXAzyTKg+cClQE+SC6rfjYj8/wW2M5LY74RLkMTh++7C4fv9NwDnXDaeMX97gnW6dOb2P55U41YawAZ9urP1Fv146vmXC57Tp2dXJk399Pjrc9+hT8+u1W9cG1Yn8bGqAbKzpGfT7ZeAA4B9I+I9ST2AiZLGN50saQtgDHB4REyW9ABwTETMkLQjcBmwR/MPkXQ0SUBlgw03rOKvk527rjyZ9Xs18tb899n3hEsY0H89dhm8GT8/7mv8/LivccG193Dl2Amc/r0v17qpq7S1Oq/GqPOO4vQLbub9Dz+qdXPqRj0lzK3mfZCLImJQ+tqX5O/ll5KmAPeTLPFpWgLUk6R3eXAaHLsAOwPj0iB7BZB3zWREjIyIIRExpGePnlX8dbKzfq9GAHp2X5uvDNuap5v1Tg7Ya3vGP/hs5u2yT3Xs0MD1532XcXdP4o6HJhc9d85bC+jbu9vy/fV7dWPOWwuq3cS2TSW+aizLG8UPJgmE26XXJt8E1kiPLQBeBf47p13v5gTYQRHx+QzbWjMfLlq8vDfy4aLFPDjxBT6/6fr8+9W5y8+56+9T2Lx/70JVWAYu/vnB/OvlN7jsLw+2eO5dE6aw3/8OZrVOHdlw/XXZdMOeRYfkq4Iqr6SpmCxv8+kKzI2ITyTtDmyUc+xjYF/gHkkfRMRfJL0k6YCIGKdkRmLriCj+v+p24K233+eQU68EYOmSpXx9zyF8YeeBHHrqlcx4ZS4NDWKD9bpzwekH1rilq66h22zCgV/ekednvMaEG5JMWudcOp7VVuvIeaccQI9uXbjxwmN47l+vsf9Jl/LCf97g1vufYeLYn7Jk6TJ+/Juxq/AMdqJORtgouXeyChUnga5Lzn4P4HagCzAJGEqSnw3S23wkNQL3AeeQ3CJ0OcnQuhMwJiLOLvaZ2203JB57fFKxU6yN6bb9CbVugpVh8YtjWbZw7kqFt8//17Yx6raHSzp3h00bn1qZZBUrq2o9yNzgmO7PA3YqcPpW6TnvAtvnlDdPhGlm7UGd9CC9ksbMMiVVbi12tTlAmlnm6iM8OkCaWS3USYR0gDSzjLWNW3hK4QBpZpmrk0uQDpBmli3hAGlmVpCH2GZmBdRLD9IP7TKzzFUqV4WkkyU9L2mqpNGS1pC0saTHJc2UdKOk1VrbTgdIM8tWqdGxhQgpqS9wEjAkfSJBB+BA4DzgwojYDHgHOLK1TXWANLPMVTCbT0eS3LMdgTWBOSR5Y29Kj18P7NPadjpAmlmmynwudg9Jk3JeRzfVExGvAeeTpEqcQ5I28SmSVIlL0tNmk+SebRVP0phZ9kqfpJlXKJuPpG7ACGBj4F1gHBVOcOMAaWaZq9BtPl8AXoqItwAk3QLsAjRK6pj2IvsBr7X2AzzENrPMSaW9WvAqMFTSmmlS7eHANOAhYP/0nMNIHufSKg6QZpa5StzmExGPk0zGPE2SYLsBGAn8BPihpJnAusDVrW2nh9hmlr0K3SgeEWcCZzYr/g+wQyXqd4A0s0w5Ya6ZWRH1ER4dIM2sFuokQjpAmlnGnDDXzKygOrkE6QBpZtlywlwzsyI8xDYzK8A9SDOzAuokPjpAmlnGSltn3SY4QJpZDdRHhHSANLNMNSXMrQcOkGaWOQ+xzcwK8G0+ZmaF1Ed8dIA0s+zVSXx0gDSzbJX4OIU2wQHSzDKnOomQDpBmlrn6CI9+aJeZ1UCFnmqY1qVGSTdJekHSdEk7Seou6T5JM9Kf3VrTTgdIM8uYSv5Toj8Ad0fE54BtgOnAacADETEAeCDdL5sDpJllqikfZCV6kJK6AruSPto1Ij6OiHeBEcD16WnXA/u0pq0OkGaWuTICZA9Jk3JeRzeramPgLeBaSc9IukrSWkDviJiTnvMG0Ls17fQkjZllrozh87yIGFLkeEdgMHBiRDwu6Q80G05HREiK1rTTPUgzy1aJvccSJ2lmA7Mj4vF0/yaSgPmmpD4A6c+5rWmqA6SZZUplvFoSEW8AsyRtkRYNB6YB44HD0rLDgNta01YPsc0se5W9EfJE4AZJqwH/AY4g6fyNlXQk8ArwjdZU7ABpZpmrZDafiHgWyHedcvjK1u0AaWaZc8JcM7NCHCDNzPJzwlwzszyaVtLUA0W06v7JNknSWyQzVu1ND2BerRthZWmv39lGEdFzZSqQdDfJ308p5kXEnivzeSujXQXI9krSpBZWE1gb4++sffCN4mZmBThAmpkV4ABZH0bWugFWNn9n7YCvQZqZFeAepJlZAQ6QZmYFOEDWkKSlkp6VNFXS7ZIaW1nP4ZIuqXDzLI+c76zp1b/Aef0lTc24eVZhDpC1tSgiBkXEVsB84PhaN8ha1PSdNb1ernWDrHocINuOfwJ9ASRtKuluSU9JekTS59Lyr0p6PH32xv2SWvWcDascSV0kPSDpaUnPSRqR55xN0u9s+0LfrbVNXovdBkjqQJK77uq0aCRwTETMkLQjcBmwB/AoMDR9xsZRwKnAj2rR5lVYZ0nPptsvAQcA+0bEe5J6ABMljW86Oc10PQY4PCImS3qA/N+ttUEOkLXV9I+tL8mzfO+T1AXYGRinT1f0r57+7AfcmD5jYzWSf6CWrUURMahpR1In4JeSdgWWkXyXTT37niSp/veLiGktfLfWBjlA1taiiBgkaU3gHpJrkNcB7+b+I8xxMXBBRIyXNAw4K5tmWhEHkwTC7SLiE0kvA2ukxxYArwL/TfKclAYKf7fWBvkaZBsQEQuBk0iGywuBlyQdAKDENumpXYHX0u3DVqjIaqErMDcNjrsDG+Uc+xjYFzhU0rci4j0Kf7fWBjlAthER8QwwBTiIpFdypKTJwPNA04X/s0iGZ0/RPlNp1aMbgCGSngMOBV7IPRgRHwJfAU6W9DUKf7fWBnmpoZlZAe5BmpkV4ABpZlaAA6SZWQEOkGZmBThAmpkV4AC5CmmWPWhceoN6a+u6TtL+6fZVkgYWOXeYpJ1b8Rkvp8v3Sipvds4HZX7WWZJOKbeN1r45QK5acrMHfQwck3tQUqtWVkXEURExrcgpw0iW2JnVFQfIVdcjwGZp7+6RNMHCNEkdJP1W0pOSpkj6Hixf9XGJpBcl3Q/0aqpI0sOShqTbe6aZbSanWW76kwTik9Pe6/9I6inp5vQznpS0S/redSXdK+l5SVeRPGO+KEm3pplxnpd0dLNjF6blD0jqmZY5m46VzGuxV0FpT3Ev4O60aDCwVUS8lAaZBRGxvaTVgcck3QtsC2wBDCRJxjANuKZZvT2BK4Fd07q6R8R8SX8EPoiI89Pz/gJcGBGPStqQZB3654EzgUcj4mxJXwaOLOHX+U76GZ2BJyXdHBFvA2sBkyLiZElnpHWfQOFMSWYrcIBcteSm6nqEJL3azsATEdGUGeiLwNZN1xdJ1hoPAHYFRkfEUuB1SQ/mqX8oMKGproiYX6AdXwAG5mS0WSfNdLMrsF/63r9JeqeE3+kkSfum2xukbX2bJLPOjWn5n4FbnE3HyuUAuWpZ1DyTTBooPswtAk6MiHuanbd3BdvRQJLX8qM8bSlZmtHoC8BOEbFQ0sN8mkmnucDZdKxMvgZpzd0DHJvmOUTS5pLWAiYA30yvUfYBds/z3onArpI2Tt/bPS1/H1g757x7gRObdiQNSjcnAN9Ky/YCurXQ1q7AO2lw/BxJD7ZJA9DUC/4WydDd2XSsLA6Q1txVJNcXn1by0KkrSEYafwVmpMdGkTwi4jMi4i3gaJLh7GQ+HeLeDuzbNElDktptSDoJNI1PZ9N/QRJgnycZar/aQlvvBjpKmg78miRAN/kQ2CH9HfYAzk7LnU3HSuZsPmZmBbgHaWZWgAOkmVkBDpBmZgU4QJqZFeAAaWZWgAOkmVkBDpBmZgX8f8j//32/dR/yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7741312741312741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.78      0.75      0.77       255\n",
      "        Fake       0.77      0.80      0.78       263\n",
      "\n",
      "    accuracy                           0.77       518\n",
      "   macro avg       0.77      0.77      0.77       518\n",
      "weighted avg       0.77      0.77      0.77       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Real', 'Fake'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
    "print(classification_report(true_labels, predictions, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f889b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
